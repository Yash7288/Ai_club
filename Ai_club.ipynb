{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52196e2b",
   "metadata": {},
   "source": [
    "Ai club task for round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00836a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/myash/.cache/kagglehub/datasets/orvile/ravdess-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"orvile/ravdess-dataset\")\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf99d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440\n"
     ]
    }
   ],
   "source": [
    "'''IMPORTING ALL THE LIBRARIES WE WILL USE'''\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from itertools import cycle\n",
    "import glob\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "sns.set_theme(style = \"white\", palette = None)\n",
    "color_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "color_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\n",
    "audio_files = glob.glob(\n",
    "    f\"{path}/**/Audio_Speech_Actors_01-24/**/*.wav\",\n",
    "    recursive=True\n",
    ")\n",
    "\n",
    "print(len(audio_files))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18052659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Tesing Zone\n",
    "# Here will test code for individual sample or small group\n",
    "# \"\"\"\n",
    "# y, sr = librosa.load(audio_files[i])\n",
    "# print(f'y: {y[:10]}')\n",
    "# print(f'shape y: {y.shape}')\n",
    "# print(sr)\n",
    "\n",
    "# pd.Series(y).plot(figsize = (10, 5), \n",
    "#                 lw = 1, \n",
    "#                 title = 'Raw Audio Example',\n",
    "#                 color = color_pal[0])\n",
    "# D = librosa.stft(y)\n",
    "# S_db = librosa.amplitude_to_db(np.abs(D), ref = np.max)\n",
    "# S_db.shape\n",
    "# fig, ax = plt.subplots(figsize = (10, 5))\n",
    "# img = librosa.display.specshow(S_db,\n",
    "#                             x_axis = 'time',\n",
    "#                             y_axis = 'log',\n",
    "#                             ax = ax)\n",
    "# #MEL Spectogram\n",
    "# S = librosa.feature.melspectrogram(y = y, sr = sr, n_mels = 128)\n",
    "# S_db_mel = librosa.amplitude_to_db(S, ref = np.max)\n",
    "# img_mel = librosa.display.specshow(S_db_mel,\n",
    "#                             x_axis = 'time',\n",
    "#                             y_axis = 'log',\n",
    "#                             ax = ax)\n",
    "\n",
    "\n",
    "# for i in range(3):  # keep small\n",
    "#     y, sr = librosa.load(audio_files[i], sr=22050)\n",
    "#     y, _ = librosa.effects.trim(y)\n",
    "\n",
    "#     print(f\"File {i} | shape: {y.shape} | sr: {sr}\")\n",
    "\n",
    "#     # Waveform\n",
    "#     fig, ax = plt.subplots(figsize=(10, 3))\n",
    "#     ax.plot(y, linewidth=1)\n",
    "#     ax.set_title(f\"Waveform {i}\")\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "\n",
    "#     # Spectrogram\n",
    "#     D = librosa.stft(y)\n",
    "#     S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(10, 4))\n",
    "#     img = librosa.display.specshow(\n",
    "#         S_db, x_axis='time', y_axis='log', ax=ax\n",
    "#     )\n",
    "#     fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
    "#     ax.set_title(f\"Spectrogram {i}\")\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "\n",
    "\n",
    "# #MEL Spectogram\n",
    "# librosa.feature.melspectogram(y, sr = sr, n_mels = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c226f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Audio silence\n",
      "Completed Adding white Noise\n",
      "Completed Pitch shift\n",
      "Completed extraction of log_mel_spectogram\n",
      "Completed pad_spectogram\n",
      "MAX_FRAMES selected: 105\n",
      "Completed full batch audio processing\n",
      "X shape: (4320, 128, 105, 1)\n",
      "y shape: (4320,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Phase 1: Preprocessing & EDA\n",
    "'''\n",
    "\n",
    "#AUDIO CLEANING\n",
    "def trim_silence(audio):\n",
    "    \"\"\"\n",
    "    Removes silence from the beginning and end of an audio signal.\n",
    "    \"\"\"\n",
    "    trimmed_audio, _ = librosa.effects.trim(audio, top_db=20)\n",
    "    return trimmed_audio\n",
    "\n",
    "\n",
    "print(\"Completed Audio silence\")\n",
    "\n",
    "#Data Augmentation Functions\n",
    "def add_white_noise(data):\n",
    "    \"\"\"\n",
    "    Adds small random white noise to the signal.\n",
    "    \"\"\"\n",
    "    noise = 0.002 * np.random.randn(len(data))\n",
    "    return data + noise\n",
    "print(\"Completed Adding white Noise\")\n",
    "\n",
    "def pitch_shift(data, sr):\n",
    "    \"\"\"\n",
    "    Shifts pitch upward by 2 semitones.\n",
    "    \"\"\"\n",
    "    return librosa.effects.pitch_shift(data, sr=sr, n_steps=2)\n",
    "print(\"Completed Pitch shift\")\n",
    "#Mel- Log Scale\n",
    "def extract_log_mel_spectrogram(audio, sr):\n",
    "    \"\"\"\n",
    "    Converts audio into a log-scaled Mel-Spectrogram.\n",
    "    \"\"\"\n",
    "    # Generate Mel-Spectrogram (power scale)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels = 128)\n",
    "\n",
    "    # Convert power spectrogram to decibel (log) scale\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec, ref = np.max, top_db = 80)\n",
    "\n",
    "    return log_mel_spec\n",
    "\n",
    "print(\"Completed extraction of log_mel_spectogram\")\n",
    "\n",
    "#Padding Function\n",
    "def pad_spectrogram(spec, max_frames):\n",
    "    if spec.shape[1] < max_frames:\n",
    "        pad_width = max_frames - spec.shape[1]\n",
    "        spec = np.pad(\n",
    "            spec,\n",
    "            ((0, 0), (0, pad_width)),\n",
    "            mode=\"constant\",\n",
    "            constant_values=-80.0  # Change spec.min() to -80.0\n",
    "        )\n",
    "    else:\n",
    "        spec = spec[:, :max_frames]\n",
    "    return spec\n",
    "def normalize_spec(spec):\n",
    "    return (spec + 80) / 80   # scale from [-80,0] → [0,1]\n",
    "\n",
    "print(\"Completed pad_spectogram\")\n",
    "frame_lengths = []\n",
    "\n",
    "for file in audio_files:\n",
    "    audio, sr = librosa.load(file, sr=22050)\n",
    "    audio = trim_silence(audio)\n",
    "    mel = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    frame_lengths.append(mel.shape[1])\n",
    "\n",
    "MAX_FRAMES = int(np.percentile(frame_lengths, 95))\n",
    "\n",
    "print(\"MAX_FRAMES selected:\", MAX_FRAMES)\n",
    "\n",
    "\n",
    "#Full Processing for a single audio file\n",
    "def process_audio_file(file_path):\n",
    "    \"\"\"\n",
    "    Load → Trim → Augment → Log-Mel → Pad\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=22050)\n",
    "    audio = trim_silence(audio)\n",
    "\n",
    "    # Original\n",
    "    spec_orig = extract_log_mel_spectrogram(audio, sr)\n",
    "    spec_orig = pad_spectrogram(spec_orig, MAX_FRAMES)\n",
    "    spec_orig = normalize_spec(spec_orig)\n",
    "   \n",
    "\n",
    "    # White noise\n",
    "    audio_noise = add_white_noise(audio)\n",
    "    spec_noise = extract_log_mel_spectrogram(audio_noise, sr)\n",
    "    spec_noise = pad_spectrogram(spec_noise, MAX_FRAMES)\n",
    "    spec_noise = normalize_spec(spec_noise)\n",
    "\n",
    "    # Pitch shift\n",
    "    audio_pitch = pitch_shift(audio, sr)\n",
    "    spec_pitch = extract_log_mel_spectrogram(audio_pitch, sr)\n",
    "    spec_pitch = pad_spectrogram(spec_pitch, MAX_FRAMES)\n",
    "    spec_pitch = normalize_spec(spec_pitch)\n",
    "\n",
    "    return spec_orig, spec_noise, spec_pitch\n",
    "print(\"Completed full batch audio processing\")\n",
    "\n",
    "#Batch Processing for a dataset folder\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for file in audio_files:\n",
    "    # RAVDESS emotion label\n",
    "    label = file.split(\"/\")[-1].split(\"-\")[2]\n",
    "\n",
    "    specs = process_audio_file(file)\n",
    "\n",
    "    for spec in specs:\n",
    "        features.append(spec)\n",
    "        labels.append(label)\n",
    "\n",
    "\n",
    "#Getting CNN ready tensors\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Add channel dimension for CNNs\n",
    "X = X[..., np.newaxis]\n",
    "\n",
    "print(\"X shape:\", X.shape)  # (samples, mel, time, 1)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# for i in range(10):\n",
    "# #Testing on One File\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     librosa.display.specshow(\n",
    "#         X[i].squeeze(), \n",
    "#         sr=22050, \n",
    "#         x_axis='time', \n",
    "#         y_axis='mel', \n",
    "#         cmap='magma', \n",
    "#         vmin=-80, \n",
    "#         vmax=0\n",
    "#     )\n",
    "#     plt.colorbar(format=\"%+2.0f dB\")\n",
    "#     plt.title(\"Log-Mel Spectrogram\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d0c25a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 290ms/step - accuracy: 0.1869 - loss: 3.1938 - val_accuracy: 0.1331 - val_loss: 28.9255\n",
      "Epoch 2/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 278ms/step - accuracy: 0.2031 - loss: 2.1477 - val_accuracy: 0.1331 - val_loss: 30.0473\n",
      "Epoch 3/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 280ms/step - accuracy: 0.2289 - loss: 2.0185 - val_accuracy: 0.1343 - val_loss: 26.1771\n",
      "Epoch 4/10\n",
      "\u001b[1m 71/108\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 276ms/step - accuracy: 0.2260 - loss: 1.9641"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     29\u001b[39m model = tf.keras.models.Sequential([\n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m#Convoluthional layer. Learn 32 filters using a 3x3 kernel\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m     tf.keras.layers.Dense(\u001b[32m8\u001b[39m, activation = \u001b[33m\"\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m ])\n\u001b[32m     57\u001b[39m model.compile(\n\u001b[32m     58\u001b[39m     optimizer = \u001b[33m\"\u001b[39m\u001b[33madam\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     59\u001b[39m     loss = \u001b[33m\"\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     60\u001b[39m     metrics = [\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     61\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_temp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\n\u001b[32m     68\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m#Evaluate neural network performace\u001b[39;00m\n\u001b[32m     71\u001b[39m model.evaluate(X_test, y_test, verbose = \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Starting to develop CNN artitecture.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_int = le.fit_transform(y)\n",
    "y_cat = tf.keras.utils.to_categorical(y_int, num_classes=8)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp, y_int_train, y_int_temp = train_test_split(\n",
    "    X, y_cat, y_int, test_size=0.2, stratify=y_int, random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_int_temp, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "input_shape = X_train.shape[1:]   # (128, MAX_FRAMES, 1)\n",
    "\n",
    "\n",
    "#Prepare data for training\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Create a convolutional neural network\n",
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    #Convoluthional layer. Learn 32 filters using a 3x3 kernel\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation = \"relu\", input_shape = input_shape),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    #Max-pooling layer, using 2x2 pool size\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),\n",
    "\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation = \"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "\n",
    "\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation = \"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    #Flatten units\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    #Add a hidden layer with dropout\n",
    "    tf.keras.layers.Dense(256, activation = \"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    #Add an output layer units for all 10 digits\n",
    "    tf.keras.layers.Dense(8, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_temp, y_temp),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "#Evaluate neural network performace\n",
    "model.evaluate(X_test, y_test, verbose = 2)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
